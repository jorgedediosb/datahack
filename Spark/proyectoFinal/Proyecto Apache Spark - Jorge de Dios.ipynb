{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb187e7c-16bf-4efd-bdf3-7bf2ce4d53c5",
     "showTitle": false,
     "title": ""
    },
    "id": "Uk5ghOBCxPlT"
   },
   "source": [
    "# Descripción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73afcd50-40e2-4e13-808c-35039a0c9e6c",
     "showTitle": false,
     "title": ""
    },
    "id": "61qYFUD1ysQX"
   },
   "source": [
    "Contamos con unos datasets que corresponden a un **listado de inspecciones de sanidad en locales** (Restaurantes, supermercados, etc), junto con su respectivo riesgo para la salud. Contamos con otro dataset que nos muestra una **descripción de dicho riesgo**.\n",
    "\n",
    "**El objetivo es cargar esos datasets bajo unas especificaciones concretas y manipularlos acorde a las instrucciones de cada ejercicio.**\n",
    "\n",
    "Todas las operaciones necesarias están descritas en los ejercicios, aunque se valorará tareas extras por propia iniciativa del alumno. También se valorará el uso del API de DataFrame\n",
    "\n",
    "**La entrega será este fichero \"ipynb\" (el nombre del fichero será vuestro nombre), junto con la imagen del plan de ejecución del ejercicio 4**, comprimido en un fichero zip con el nombre y apellidos del alumno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a11d615-dcf1-4074-97a0-c07ea36e0d5d",
     "showTitle": false,
     "title": ""
    },
    "id": "8Z0h3dF9Vg4X"
   },
   "source": [
    "# Descargar Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b13db421-61d6-43df-9aba-16a9d29d9997",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "curl -O 'https://raw.githubusercontent.com/masfworld/datahack_docker/master/zeppelin/data/food_inspections_lite.csv'\n",
    "curl -O 'https://raw.githubusercontent.com/masfworld/datahack_docker/master/zeppelin/data/risk_description.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9992d60-96b6-43cd-8e2d-9c52dd718662",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp('file:/databricks/driver/food_inspections_lite.csv','dbfs:/dataset/food_inspections_lite.csv')\n",
    "dbutils.fs.cp('file:/databricks/driver/risk_description.csv','dbfs:/dataset/risk_description.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a29800e4-d003-4d6e-966d-0e98d8fff444",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls('/dataset/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c57ec8fd-5255-4a35-b3a0-a1cfd42dae9b",
     "showTitle": false,
     "title": ""
    },
    "id": "prvVhMD4a5o7"
   },
   "source": [
    "# Ejercicio 1\n",
    "---\n",
    "\n",
    "1. **Crea dos dataframes, uno a partir del fichero `food_inspections_lite.csv` y otro a partir de `risk_description.csv`**\n",
    "2. **Convierte esos dos dataframes a tablas delta**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f933ce7-e877-4189-b7bc-8fe37300bbf4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Eliminación de las tablas para evitar conflictos con posibles datos antiguos\n",
    "\n",
    "# Definir la ruta de las tablas Delta\n",
    "delta_food_inspections_path = \"dbfs:/delta/food_inspections\"\n",
    "delta_risk_description_path = \"dbfs:/delta/risk_description\"\n",
    "\n",
    "# Eliminar las tablas Delta si ya existen\n",
    "spark.sql(f\"DROP TABLE IF EXISTS delta.`{delta_food_inspections_path}`\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS delta.`{delta_risk_description_path}`\")\n",
    "\n",
    "# Crear las tablas Delta nuevamente\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS food_inspections USING DELTA LOCATION '{delta_food_inspections_path}'\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS risk_description USING DELTA LOCATION '{delta_risk_description_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cf7f99b-fc03-49b3-90b0-1458a7555f38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imprimir el contenido de los ficheros food_inspections_lite.csv y risk_description.csv para comprobar los datos\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Definir las rutas de los archivos CSV\n",
    "food_inspections_path = \"file:/databricks/driver/food_inspections_lite.csv\"\n",
    "risk_description_path = \"file:/databricks/driver/risk_description.csv\"\n",
    "\n",
    "# Leer los ficheros CSV\n",
    "df_food_inspections = pd.read_csv(food_inspections_path)\n",
    "df_risk_description = pd.read_csv(risk_description_path)\n",
    "\n",
    "# Imprimir primeras filas de los ficheros\n",
    "print(df_food_inspections.head())\n",
    "print(df_risk_description.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daff7c65-9f34-4e27-a1df-a6f8d86d80cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rutas de los ficheros CSV almacenados en DBFS (Databricks File System)\n",
    "food_inspections_path = \"dbfs:/dataset/food_inspections_lite.csv\"\n",
    "risk_description_path = \"dbfs:/dataset/risk_description.csv\"\n",
    "\n",
    "# Crear el DataFrame a partir del fichero food_inspections_lite.csv\n",
    "df_food_inspections = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(food_inspections_path)\n",
    "\n",
    "# Crear el DataFrame a partir del fichero risk_description.csv\n",
    "df_risk_description = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(risk_description_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52a75246-6d0b-4b96-99f6-2fb2e92ff9c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imprimir Dataframes\n",
    "df_food_inspections.show()\n",
    "df_risk_description.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b5f4897-a1a2-4512-a3d6-d355b19bf413",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imprimir esquemas de los DataFrames\n",
    "df_food_inspections.printSchema()\n",
    "df_risk_description.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fab8007-d10c-4855-8497-0cb88ce1c88c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Limpieza del nombre de las columnas por seguridad, compatibilidad y estabilidad.\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def clean_column_names(df):\n",
    "    for col_name in df.columns:\n",
    "        # Convertir a minúsculas\n",
    "        cleaned_name = col_name.lower()\n",
    "        # Reemplazar espacios y caracteres especiales\n",
    "        cleaned_name = cleaned_name.strip().replace(' ', '_').replace(';', '').replace('{', '').replace('}', '') \\\n",
    "                                            .replace('(', '').replace(')', '').replace('\\n', '').replace('\\t', '') \\\n",
    "                                            .replace('=', '').replace('#', '')\n",
    "        # Reglas específicas para columnas conocidas\n",
    "        if cleaned_name == 'license_':\n",
    "            cleaned_name = 'license'\n",
    "        df = df.withColumnRenamed(col_name, cleaned_name)\n",
    "    return df\n",
    "\n",
    "df_food_inspections_cleaned = clean_column_names(df_food_inspections)\n",
    "df_risk_description_cleaned = clean_column_names(df_risk_description)\n",
    "\n",
    "print(\"Columnas originales df_food_inspections:\")\n",
    "print(df_food_inspections.columns)\n",
    "\n",
    "print(\"Columnas df_food_inspections después de la limpieza:\")\n",
    "print(df_food_inspections_cleaned.columns)\n",
    "\n",
    "print(\"Columnas originales df_risk_description:\")\n",
    "print(df_risk_description.columns)\n",
    "\n",
    "print(\"Columnas df_risk_description después de la limpieza:\")\n",
    "print(df_risk_description_cleaned.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fa3c361-17c8-4daa-a1d7-093883d45950",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 2º: Convertir y escribir los DataFrames (limpios) en tablas Delta:\n",
    "\n",
    "# Especificar las rutas de salida para las tablas Delta\n",
    "delta_food_inspections_path = \"dbfs:/delta/food_inspections\"\n",
    "delta_risk_description_path = \"dbfs:/delta/risk_description\"\n",
    "\n",
    "# Convertir y escribir el DataFrame food_inspections limpio como tabla Delta\n",
    "df_food_inspections_cleaned.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(delta_food_inspections_path)\n",
    "\n",
    "# Convertir y escribir el DataFrame risk_description limpio como tabla Delta\n",
    "df_risk_description_cleaned.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(delta_risk_description_path)\n",
    "\n",
    "# Imprimir esquemas\n",
    "df_food_inspections_cleaned.printSchema()\n",
    "df_risk_description_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d8270ec-8184-4944-a62f-d106bf758f73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Registrar tablas Delta en Spark SQL (para realizar consultas SQL sin especificar la ruta en cada operación (entre otros motivos))\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS food_inspections USING DELTA LOCATION 'dbfs:/delta/food_inspections'\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS risk_description USING DELTA LOCATION 'dbfs:/delta/risk_description'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ced6c10-e62b-46f2-89c9-97ba15ebb977",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Consultar la tabla food_inspections\n",
    "spark.sql(\"SELECT * FROM food_inspections LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18208f6c-93e4-4fe9-a6a7-252e1180f88b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Consultar la tabla risk_description\n",
    "spark.sql(\"SELECT * FROM risk_description\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d26822-a808-45a1-9f43-67a659891203",
     "showTitle": false,
     "title": ""
    },
    "id": "4-HOEezxVnCe"
   },
   "source": [
    "# Ejercicio 2\n",
    "**Obtén el número de inspecciones distintas con Riesgo alto `Risk 1 (High)`**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f14b6851-aa3c-4c27-9afe-8af5de08d089",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Opción 1 resolución ejercicio haciendo un filtrado por con 'Risk 1 (High)' en el dataset 'food_inspections' columna 'Risk' y contar las inspecciones distintas según su ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54f562ac-5927-4192-9a39-0437a60f93b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer el DataFrame desde Delta Lake\n",
    "df_food_inspections = spark.read.format(\"delta\").load(\"dbfs:/delta/food_inspections\")\n",
    "\n",
    "# Filtrar las inspecciones con riesgo alto (Risk 1 (High))\n",
    "df_high_risk_inspections = df_food_inspections.filter(col('risk') == 'Risk 1 (High)')\n",
    "\n",
    "# Inspecciones distintas basadas en inspection_id\n",
    "distinct_high_risk_inspections = df_high_risk_inspections.select('inspection_id').distinct().count()\n",
    "\n",
    "# Número total de filas en el DataFrame original\n",
    "total_rows = df_food_inspections.count()\n",
    "print(f\"Filas totales en 'food inspections': {total_rows}\")\n",
    "\n",
    "# Mostrar el número de inspecciones distintas con riesgo alto\n",
    "print(f\"Número de inspecciones distintas con 'Risk 1 (High)': {distinct_high_risk_inspections}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a44fbd-c6e4-461b-9970-4f5ac592cc9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Opción 2 (más rápida) sin comprobar si las inspecciones son distintas (entendiendo que si el dataset tiene un ID único por fila, todas serán distintas).\n",
    "El resultado es el mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dca60b59-410c-493a-a3da-801e2c91a9bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtrar las filas con 'Risk 1 (High)'\n",
    "df_risk_high = df_food_inspections.filter(df_food_inspections['Risk'] == 'Risk 1 (High)')\n",
    "\n",
    "# Número total de filas en el DataFrame original\n",
    "total_rows = df_food_inspections.count()\n",
    "print(f\"Filas totales en 'food inspections': {total_rows}\")\n",
    "\n",
    "# Contar el número de filas filtradas\n",
    "count_risk_high = df_risk_high.count()\n",
    "print(f\"Número de filas con 'Risk 1 (High)': {count_risk_high}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00285d5a-3d67-4da4-9772-ce6df67aae09",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Nota del ejercicio: Al comprobar los nombres únicos en la columna 'Risk' de 'food_inspections' para identificar cuantos restaurantes tienen 'Risk 1 (High)' por si hubiese datos distintos a los indicados en 'df_risk_description', se comprueba que SI LOS HAY (a parte de los riesgos 1, 2 y 3 hay valores 'null' y 'All')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5497cbf5-c59a-413d-943d-29ed68c5bed1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Valores únicos de la columna 'Risk' en df_food_inspections\n",
    "df_food_inspections.select(\"risk\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df60ba61-545a-4ebc-afbd-a4743dd3f854",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Se comprueba el nº de filas que contienen 'null' y 'all' en la columna 'Risk' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2be96405-cfa3-42fb-9ba3-e44f4b3e7732",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtrar y mostrar las filas con valores 'null' en la columna 'Risk'\n",
    "df_null_risk = df_food_inspections.filter(df_food_inspections[\"risk\"].isNull())\n",
    "\n",
    "# Número total de filas con valores 'null'\n",
    "total_null_rows = df_null_risk.count()\n",
    "df_null_risk.show()\n",
    "print(f\"Filas totales con valor 'null' en columna 'risk': {total_null_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7719893-df54-4d72-b4d4-309119e45f5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtrar y mostrar las filas con valor 'All' en la columna 'Risk'\n",
    "df_all_risk = df_food_inspections.filter(df_food_inspections[\"risk\"] == \"All\")\n",
    "\n",
    "# Número total de filas con valores 'all'\n",
    "total_all_rows = df_all_risk.count()\n",
    "df_all_risk.show()\n",
    "print(f\"Filas totales con valor 'all' en columna 'risk': {total_all_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "618f5d19-5d03-4da7-b170-6c05a4c60fd0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "El resultado obtenido es:\n",
    "- Nº de filas con valores 'null': 5\n",
    "- Nº de filas con valores 'All': 9\n",
    "\n",
    "El nº de filas no supone un cambio significativo al suponer un 0.05% y 0.09% respectivamente, pero se procede a la limpieza del dataframe para tener mayor consistencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83982eb0-1fe4-4184-8db6-4fd15ccdbe3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Limpieza de la columna 'Risk':\n",
    "# 1. Reemplazar los valores All por Risk 1 (High)\n",
    "# 2. Eliminar las filas con valores null\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Cargar la tabla Delta de food inspections\n",
    "df_food_inspections = spark.read.format(\"delta\").load(\"dbfs:/delta/food_inspections\")\n",
    "\n",
    "# Reemplazar los valores 'All' por 'Risk 1 (High)'\n",
    "df_food_inspections_cleaned = df_food_inspections.withColumn(\n",
    "    \"risk\", when(col(\"risk\") == \"All\", \"Risk 1 (High)\").otherwise(col(\"risk\")))\n",
    "\n",
    "# Eliminar las filas con valores null en la columna 'risk'\n",
    "df_food_inspections_cleaned = df_food_inspections_cleaned.filter(col(\"risk\").isNotNull())\n",
    "\n",
    "# Mostrar los valores únicos de la columna 'risk' después de la limpieza\n",
    "print(\"Valores únicos en la columna 'risk' después de la limpieza:\")\n",
    "df_food_inspections_cleaned.select(\"risk\").distinct().show()\n",
    "\n",
    "# Contar las inspecciones con 'Risk 1 (High)'\n",
    "df_risk_high = df_food_inspections_cleaned.filter(col(\"risk\") == \"Risk 1 (High)\")\n",
    "num_risk_all = df_risk_high.count()\n",
    "\n",
    "print(f\"Número de inspecciones distintas con 'Risk 1 (High)' añadiendo los 'All': {num_risk_all}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea37802e-9b95-49d0-a584-f24b7fda1b9b",
     "showTitle": false,
     "title": ""
    },
    "id": "3R1kpKmIXi-h"
   },
   "source": [
    "\n",
    "# Ejercicio 3\n",
    "**A partir de los dataframes cargados anteriormente, obtén una tabla con las siguientes columnas:<br>**\n",
    "1. `DBA Name`\n",
    "2. `Facility Type`\n",
    "3. `Risk`\n",
    "4. `Risk description`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94f64492-0f1d-4a7a-999e-d5805868ce3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Realizar join utilizando expresiones para alinear los valores\n",
    "df_joined = df_food_inspections_cleaned.join(df_risk_description_cleaned, df_food_inspections_cleaned['risk'].contains('Risk'), how='inner')\n",
    "\n",
    "# Seleccionar las columnas necesarias\n",
    "df_result = df_joined.select(\n",
    "  col('dba_name').alias('DBA Name'),\n",
    "  col('facility_type').alias('Facility Type'),\n",
    "  col('risk').alias('Risk'),\n",
    "  col('description').alias('Risk description')\n",
    "  )\n",
    "\n",
    "# Mostrar el resultado\n",
    "df_result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b089ed46-7d87-4edd-89f3-ad411ef5191c",
     "showTitle": false,
     "title": ""
    },
    "id": "ZX1ahdYyb2L9"
   },
   "source": [
    "# Ejercicio 4\n",
    "**Accede a la Spark UI para ver el plan de ejecución del ejercicio anterior (ejercicio 3). Describe cada una de las piezas/cajas que componen el plan de ejecución (Una descripción breve de una línea por caja será suficiente).**<br><br>**Recordad hacer un pantallazo del plan de ejecución analizado y enviadlo junto con este notebook**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f22dd08-347f-451c-a9da-542ed17e0b78",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Descripción de cada caja en el plan de ejecución del ejercicio 3 en donde se realizó un join entre dos dataframes (df_food_inspections_cleaned y df_risk_description_cleaned)**\n",
    "\n",
    "**Stage 211 (skipped)**\n",
    "\n",
    "- Scan csv: Spark escanea y lee los datos desde un archivo CSV.\n",
    "\n",
    "- WholeStageCodegen: Optimizador que ayuda a reducir la sobrecarga y mejorar la eficiencia en el procesamiento de los datos leídos del CSV.\n",
    "\n",
    "- Exchange: Redistribuye los datos ente los nodos para preparar la operación de join, generando una etapa de intercambio.\n",
    "\n",
    "**Stage 212**\n",
    "\n",
    "- Scan csv: Similar a la caja en el Stage 211, esta operación escanea los datos desde la fuente CSV.\n",
    "\n",
    "- ShuffleQueryStage: Recibe datos redistribuidos (shuffle) del Exchange en el Stage 211 para la operación de join.\n",
    "\n",
    "- WholeStageCodegen: Al igual que en el Stage 211, esta caja optimiza la ejecución de los datos combinados y finaliza la ejecución del join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fff4369c-a922-4910-a85d-d4e0b7b28803",
     "showTitle": false,
     "title": ""
    },
    "id": "2sQNdQ2Gbz4p"
   },
   "source": [
    "# Ejercicio 5\n",
    "**1. Para cada local (columna `DBA Name`) y su resultado (columna `Results`), obtén el número de inspecciones que ha tenido**<br><br>\n",
    "**2. Obtén los dos locales (`DBA Name`) que más inspecciones han tenido por cada uno de los resultados**<br><br>\n",
    "**3. Guarda los resultados del punto 2 en una nueva tabla Delta llamada `inspections_results`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dc5acf0-bda0-4605-9ead-f628ac9908f0",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLFgHfWp0pbL",
    "outputId": "0c1d2ade-e00c-4ffa-f1fd-ac650f7d4ad6"
   },
   "outputs": [],
   "source": [
    "# Paso 1: Contar el número de inspecciones por local y resultado\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "# Contar el número de inspecciones por local y resultado\n",
    "df_count_inspections = df_food_inspections_cleaned.groupBy(\"dba_name\", \"results\") \\\n",
    "    .agg(count(\"*\").alias(\"total_inspections\")) \\\n",
    "    .orderBy(col(\"total_inspections\").desc())\n",
    "\n",
    "# Mostrar para verificar\n",
    "df_count_inspections.show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58e87898-35cc-46fc-b1f7-87dc24466114",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Número de inspecciones totales agrupadas por restaurantes\n",
    "df_count_inspections = df_food_inspections_cleaned.groupBy(\"dba_name\").agg(count(\"*\").alias(\"total_inspections\"))\n",
    "df_inspection_counts_sorted = df_count_inspections.orderBy(col(\"total_inspections\").desc())\n",
    "df_inspection_counts_sorted.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d331e6-07d0-4e4a-ae8f-cead23754cd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, row_number, count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Paso 2: Obtener los dos locales (DBA Name) que más inspecciones han tenido por cada uno de los resultados\n",
    "\n",
    "# Contar el nº de inspecciones por 'dba_name' y 'results'\n",
    "df_count_inspections = df_food_inspections.groupBy('dba_name', 'results').agg(count('*').alias('total_inspections'))\n",
    "\n",
    "# Definir una ventana para particionar por 'results' y ordenar por 'total_inspections' en orden descendente\n",
    "window_spec = Window.partitionBy(\"results\").orderBy(col(\"total_inspections\").desc())\n",
    "\n",
    "# Agregar una columna con el número de fila dentro de cada partición\n",
    "df_ranked = df_count_inspections.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "# Filtrar para obtener solo las dos primeras filas en cada partición\n",
    "df_top2 = df_ranked.filter(col(\"rank\") <= 2).drop(\"rank\")\n",
    "\n",
    "# Mostrar el resultado\n",
    "df_top2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44b2f98c-30f3-48a7-9a7c-1998363a7806",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 3: Guardar los resultados en una nueva tabla Delta\n",
    "\n",
    "# Guardar los resultados en una nueva tabla Delta llamada inspections_results\n",
    "df_top2.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"dbfs:/delta/inspections_results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1170c38-6f94-451b-ab32-81871e52c1c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verificar el esquema del DataFrame df_top2\n",
    "df_top2.printSchema()\n",
    "\n",
    "# Verificar el esquema de la tabla Delta existente\n",
    "spark.read.format(\"delta\").load(\"dbfs:/delta/inspections_results\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42d86456-6a93-4b5d-aed1-dd2105921c8d",
     "showTitle": false,
     "title": ""
    },
    "id": "04y2Ys6L0wTU"
   },
   "source": [
    "# Ejercicio 6\n",
    "1. **Actualiza la tabla delta del ejercicio anterior `inspections_results`, especificando `DBA_Name = error`**<br>\n",
    "2. **Restaura la tabla a su estado original**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8987f1d-b2cc-4ce8-b6b6-3fdfb430ae8e",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1Pp9VwU1DnK",
    "outputId": "b56672fa-5a19-4f6a-81de-522b931e433f"
   },
   "outputs": [],
   "source": [
    "# Paso 1: Actualizar la tabla Delta inspections_results\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Leer la tabla Delta inspections_results\n",
    "df_inspections_results = spark.read.format(\"delta\").load(\"dbfs:/delta/inspections_results\")\n",
    "\n",
    "# Mostrar algunos registros antes de la actualización\n",
    "print(\"Registros antes de la actualización:\")\n",
    "df_inspections_results.show()\n",
    "\n",
    "# Actualizar la columna DBA_Name con el valor 'error'\n",
    "df_updated = df_inspections_results.withColumn(\"dba_name\", lit(\"error\"))\n",
    "\n",
    "# Guardar los cambios en la misma tabla Delta 'inspections_results'\n",
    "df_updated.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/delta/inspections_results\")\n",
    "\n",
    "# Mostrar registros después de la actualización\n",
    "print(\"Registros después de la actualización:\")\n",
    "df_updated.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e58252ae-ca19-4baf-808b-bcfa4a7e91bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 2: Restaura la tabla a su estado original\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Cargar la tabla Delta\n",
    "delta_table_path = \"dbfs:/delta/inspections_results\"\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "\n",
    "# Mostrar el historial de versiones de la tabla\n",
    "history_df = delta_table.history()\n",
    "history_df.display(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ae66648-b6f4-481e-8826-bf2ce28a9534",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 2º (continuación): Restaurar la tabla a una versión anterior (0), la versión original\n",
    "previous_version = 0\n",
    "\n",
    "# Restaurar la tabla a su estado original (versión 0)\n",
    "spark.sql(f\"RESTORE TABLE delta.`{delta_table_path}` TO VERSION AS OF {previous_version}\")\n",
    "\n",
    "# Verificar que la restauración fue exitosa mostrando algunos registros\n",
    "df_restored = spark.read.format(\"delta\").load(delta_table_path)\n",
    "df_restored.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a86c2cf1-c6e3-47fa-8677-f2bc8699b5e7",
     "showTitle": false,
     "title": ""
    },
    "id": "77M6b7WwsTA1"
   },
   "source": [
    "# Ejercicio 7\n",
    "\n",
    "**Crea una aplicación con Structured Streaming que lea los datos del topic de Kafka `inspections`. La url del servidor Kafka es `35.237.99.179:9094`:**\n",
    "\n",
    "**Los datos procedentes de este topic son exactamente los mismos que estamos analizando durante todo este notebook, `Food Inspections`, así que el esquema es el mismo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "023e5689-a6a8-4864-a5e1-7fa5d6185718",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import from_json, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0343e58-5d78-411f-9eca-1c95772b6a23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lectura desde Kafka (origen) para verificación de datos\n",
    "\n",
    "# Configuración servidor y topic de Kafka\n",
    "kafka_server = \"35.237.99.179:9094\"\n",
    "kafka_topic = \"inspections\"\n",
    "\n",
    "df_kafka_origin = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_server) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Mostrar registros\n",
    "df_kafka_origin.selectExpr(\"CAST(value AS STRING)\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8534256e-a173-48cb-9ea8-019d2912494a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuración de Streaming desde Kafka\n",
    "df_kafka = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_server) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .load()\n",
    "\n",
    "# Definir el esquema para los datos de las inspecciones\n",
    "schema = StructType([\n",
    "    StructField(\"Inspection ID\", StringType(), True),\n",
    "    StructField(\"DBA Name\", StringType(), True),\n",
    "    StructField(\"AKA Name\", StringType(), True),\n",
    "    StructField(\"License #\", StringType(), True),\n",
    "    StructField(\"Facility Type\", StringType(), True),\n",
    "    StructField(\"Risk\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"Zip\", StringType(), True),\n",
    "    StructField(\"Inspection Date\", StringType(), True),\n",
    "    StructField(\"Inspection Type\", StringType(), True),\n",
    "    StructField(\"Results\", StringType(), True),\n",
    "    StructField(\"Violations\", StringType(), True),\n",
    "    StructField(\"Latitude\", StringType(), True),\n",
    "    StructField(\"Longitude\", StringType(), True),\n",
    "    StructField(\"Location\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_kafka.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd335fb9-cec0-4df8-a77a-198118ac7c63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convertir los datos desde Kafka usando el esquema definido\n",
    "dataset = df_kafka.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"timestamp\") \\\n",
    "    .withColumn(\"value\", from_json(\"value\", schema)) \\\n",
    "    .select(col('key'), col(\"timestamp\"), col('value.*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76cf2699-07dd-41e5-8f5e-1fe2fc80abff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Opción 1 - Mostrar dataset con 'display'\n",
    "# Permite ver los datos en tiempo real de forma sencilla pero aporta menos versatilidad y control\n",
    "# Sólo válido en entornos de pruebas, NO USAR EN ENTORNOS DE PRODUCCIÓN\n",
    "\n",
    "dataset.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a950cfe9-b3dc-42b0-b46b-db8d4db24a67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Opción 2 - Mostrar dataset con 'writeStream'\n",
    "# Almacena los resultados en la tabla temporal 'inspections_topic' permitiendo mayor versatilidad y control\n",
    "# Para ver los resultados, esperar a que estén almacenados y ejecutar la consulta SQL\n",
    "\n",
    "# Escribir los datos en memoria\n",
    "dataset.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .queryName(\"inspections_topic\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f99531ca-111c-4eb5-a96b-e66485e5d5dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Consultar dataset\n",
    "SELECT * FROM inspections_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "090c9d0b-305b-432c-a686-e87b3fca83be",
     "showTitle": false,
     "title": ""
    },
    "id": "JbyutPORhr0b"
   },
   "source": [
    "# Ejercicio 8\n",
    "**En base a la fuente de datos del ejercicio anterior, obtén cada 5 segundos el número de inspecciones por `Facility Type`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09b2abff-8130-438e-9946-66f0627e2c9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Opción 1 - Mostrar dataset con 'display'\n",
    "# Permite ver los datos en tiempo real de forma sencilla pero aporta menos versatilidad y control\n",
    "# Sólo válido en entornos de pruebas, NO USAR EN ENTORNOS DE PRODUCCIÓN\n",
    "\n",
    "from pyspark.sql.functions import col, window\n",
    "(\n",
    "dataset\n",
    "  .groupBy(window(col(\"timestamp\"), \"5 seconds\"), col(\"Facility Type\"))\n",
    "  .count()\n",
    "  .display()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84a3cb7a-4e14-4642-a908-31cee07e6859",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Opción 2 - Mostrar dataset con 'writeStream'\n",
    "# Almacena los resultados en la tabla temporal 'inspections_update_topic' permitiendo mayor versatilidad y control\n",
    "# Para ver los resultados, esperar a que estén almacenados y ejecutar la consulta SQL\n",
    "\n",
    "from pyspark.sql.functions import col, window\n",
    "(\n",
    "dataset\n",
    "  .groupBy(window(col(\"timestamp\"), \"5 seconds\"), col(\"Facility Type\"))\n",
    "  .count()\n",
    "  .writeStream \\\n",
    "  .outputMode(\"update\") \\\n",
    "  .format(\"memory\") \\\n",
    "  .option(\"truncate\", \"false\") \\\n",
    "  .queryName(\"inspections_update_topic\") \\\n",
    "  .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a9f429d-4f08-4bfb-b179-acc4f2bc730c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Consultar dataset\n",
    "select * from inspections_update_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86fcae80-04aa-4d20-892b-b813877a2b60",
     "showTitle": false,
     "title": ""
    },
    "id": "sr3vImmwxQiN"
   },
   "source": [
    "# Ejercicio 9\n",
    "**En base a la fuente de datos del ejercicio 7, obtén cada 5 segundos el número de inspecciones por `Results` de los últimos 30 segundos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5aa5ea2-99b5-462a-904f-5a5d504ba4e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Opción 1 - Mostrar dataset con 'display'\n",
    "# Permite ver los datos en tiempo real de forma sencilla pero aporta menos versatilidad y control\n",
    "# Sólo válido en entornos de pruebas, NO USAR EN ENTORNOS DE PRODUCCIÓN\n",
    "\n",
    "from pyspark.sql.functions import col, window\n",
    "(\n",
    "dataset\n",
    "  .groupBy(window(col(\"timestamp\"), \"30 seconds\", \"5 seconds\"), col(\"results\"))\n",
    "  .count()\n",
    "  .display()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "258fde42-bc60-4ffd-8608-3e411e386743",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Opción  2 - Usando 'writeStream'\n",
    "# Almacena los resultados en la tabla temporal 'results_update_topic' permitiendo mayor versatilidad y control\n",
    "# Para ver los resultados, esperar a que estén almacenados y ejecutar la consulta SQL\n",
    "\n",
    "from pyspark.sql.functions import col, window\n",
    "(\n",
    "dataset\n",
    "  .groupBy(window(col(\"timestamp\"), \"30 seconds\", \"5 seconds\"), col(\"results\"))\n",
    "  .count()\n",
    "  .writeStream \\\n",
    "  .outputMode(\"update\") \\\n",
    "  .format(\"memory\") \\\n",
    "  .option(\"truncate\", \"false\") \\\n",
    "  .queryName(\"results_update_topic\") \\\n",
    "  .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eea928e-3a4f-45a7-8ca9-abd527b39f7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Mostrar los resultados del dataset\n",
    "select * from results_update_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14d4e4b5-b08f-41f9-a34b-7cf40f62b7c4",
     "showTitle": false,
     "title": ""
    },
    "id": "BNea5uVAx1DG"
   },
   "source": [
    "# Ejercicio 10\n",
    "1. **Actualiza la columna `Results` de la tabla delta de food inspections creada en el ejercicio 1 al valor `No result`**\n",
    "2. **Actualiza los datos de la tabla modificada en el punto 1 conforme vayan llegando elementos en Kafka**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5996777d-c73c-4532-a477-e42d22ec4f04",
     "showTitle": false,
     "title": ""
    },
    "id": "fbT-_TJv0YMj"
   },
   "source": [
    "Se aconseja parar todos los streams anteriores ya que el de este ejercicio suele hacer un uso intensivo de los recursos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c68dfaf-72a8-46ec-a22d-2bdab5181262",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 1.1\n",
    "\n",
    "# Cargar la tabla Delta de food inspections\n",
    "df_food_inspections = spark.read.format(\"delta\").load(\"dbfs:/delta/food_inspections\")\n",
    "\n",
    "# Mostrar tabla\n",
    "df_food_inspections.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7dc30c0-de9d-45d9-bd50-17e802cccec3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso  1.2 (continuación):\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Actualizar la columna Results con el valor 'No result'\n",
    "df_updated_results = df_food_inspections.withColumn(\"results\", lit(\"No result\"))\n",
    "\n",
    "# Sobrescribir la tabla Delta con los nuevos valores\n",
    "df_updated_results.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/delta/food_inspections\")\n",
    "\n",
    "# Mostrar registros después de la actualización\n",
    "df_updated_results.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cfdba9b-c8ef-46fa-b555-452798d64170",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 2.1 - Actualiza los datos de la tabla modificada en el punto 1 conforme vayan llegando elementos en Kafka\n",
    "\n",
    "# Configuración servidor y topic de Kafka\n",
    "kafka_server = \"35.237.99.179:9094\"\n",
    "kafka_topic = \"inspections\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c69a0d02-de06-42be-9018-993153ef9c24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 2.2\n",
    "\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Definir el esquema para los datos de las inspecciones\n",
    "schema = StructType([\n",
    "    StructField(\"inspection_id\", StringType(), True),\n",
    "    StructField(\"dba_name\", StringType(), True),\n",
    "    StructField(\"aka_name\", StringType(), True),\n",
    "    StructField(\"license\", StringType(), True),\n",
    "    StructField(\"facility_type\", StringType(), True),\n",
    "    StructField(\"risk\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip\", StringType(), True),\n",
    "    StructField(\"inspection_date\", StringType(), True),\n",
    "    StructField(\"inspection_type\", StringType(), True),\n",
    "    StructField(\"results\", StringType(), True),\n",
    "    StructField(\"violations\", StringType(), True),\n",
    "    StructField(\"latitude\", StringType(), True),\n",
    "    StructField(\"longitude\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Leer el stream de Kafka\n",
    "df_kafka = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_server) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .load()\n",
    "\n",
    "# Convertir los datos de Kafka (json) al esquema estructurado\n",
    "df_stream = df_kafka.selectExpr(\"CAST(value AS STRING) as json_string\") \\\n",
    "    .withColumn(\"data\", from_json(col(\"json_string\"), schema)) \\\n",
    "    .select(\"data.*\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46c2d202-fcdd-427f-8c96-cdde3d0b733e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 2.3\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Función para actualizar la tabla Delta\n",
    "def upsert_to_delta(microBatchOutputDF, batchId):\n",
    "    # Mostrar el número de registros procesados en el batch actual\n",
    "    print(f\"Processing batch {batchId} with {microBatchOutputDF.count()} records\")\n",
    "    \n",
    "    # Mostrar algunos registros del batch actual para debugging\n",
    "    microBatchOutputDF.show(truncate=False)\n",
    "    \n",
    "    # Cargar la tabla Delta existente\n",
    "    delta_table = DeltaTable.forPath(spark, \"dbfs:/delta/food_inspections\")\n",
    "    \n",
    "    # Realizar la operación de merge en la tabla Delta\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        microBatchOutputDF.alias(\"source\"),\n",
    "        \"target.inspection_id = source.inspection_id\"\n",
    "    ).whenMatchedUpdate(set={\n",
    "        \"dba_name\": col(\"source.dba_name\"),\n",
    "        \"aka_name\": col(\"source.aka_name\"),\n",
    "        \"license\": col(\"source.license\"),\n",
    "        \"facility_type\": col(\"source.facility_type\"),\n",
    "        \"risk\": col(\"source.risk\"),\n",
    "        \"address\": col(\"source.address\"),\n",
    "        \"city\": col(\"source.city\"),\n",
    "        \"state\": col(\"source.state\"),\n",
    "        \"zip\": col(\"source.zip\"),\n",
    "        \"inspection_date\": col(\"source.inspection_date\"),\n",
    "        \"inspection_type\": col(\"source.inspection_type\"),\n",
    "        \"results\": col(\"source.results\"),\n",
    "        \"violations\": col(\"source.violations\"),\n",
    "        \"latitude\": col(\"source.latitude\"),\n",
    "        \"longitude\": col(\"source.longitude\"),\n",
    "        \"location\": col(\"source.location\")\n",
    "    }).whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb380805-c2f6-4c66-9e4e-2c6b91d49d67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 2.4\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "# Configurar el stream con un trigger para procesar los datos disponibles\n",
    "query = df_stream.writeStream \\\n",
    "    .foreachBatch(upsert_to_delta) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"dbfs:/delta/checkpoints/food_inspections\") \\\n",
    "    .trigger(once=True) \\\n",
    "    .start()\n",
    "\n",
    "# Verificar los datos en la tabla Delta\n",
    "spark.sql(\"SELECT * FROM delta.`dbfs:/delta/food_inspections`\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54e4c169-732d-4163-8e17-d18ef12cc95d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Ejercicio 11: Diseño de Arquitectura de Datos\n",
    "\n",
    "**Objetivo:** Diseñar una arquitectura de datos para manejar el sistema de inspecciones de sanidad descrito en los datasets utilizados en este proyecto.\n",
    "\n",
    "**Descripción:**\n",
    "Queremos que diseñen una arquitectura de datos eficiente y escalable para el sistema de inspecciones de sanidad. Los elementos a tener en cuenta:\n",
    "* Para obtener los datos (CSVs incluidos en este proyecto) será necesario contectarse a un API REST\n",
    "* Necesitamos limpiar los datos ya que en muchas veces vienen incompletos o con campos que no corresponden con la realidad\n",
    "* Tenemos que cotejar las empresas con las empresas dadas de alta en el sistema, las cuales están almacenadas en un directorio en dbfs, en formato delta lake.\n",
    "* En paralelo debemos diseñar un sistema de registro para los inspectores, donde podrán administrar y ver las inspecciones realizadas\n",
    "* Que calcular algunas métricas a nivel de inspección tales como: Empresa con más inspecciones fallidas, o porcentaje de empresas con un mínimo de inspecciones. Estas métricas deberán actualizarse con un máximo de latencia de 30 minutos\n",
    "* También definiremos métricas a nivel de inspectores, como inspectores con más inspecciones, ... Estás métricas sólo serán visibles a ciertos usuarios de la administración.\n",
    "* Algunas de las métricas de inspecciones serán expuestas en un dashboard accesible por ciertos cargos del gobierno.\n",
    "* Además debemos exponer los datos de inspecciones (con ciertas restricciones de visibilidad) a todos los ciudadanos a través de un API.\n",
    "\n",
    "Este diseño debe abordar los siguientes puntos:\n",
    "\n",
    "1. **Dominio de Datos:** Identifica y define dominios de datos relevantes para el sistema de inspecciones de sanidad.\n",
    "2. **Propietarios del Dominio:** Asigna roles y responsabilidades para cada dominio de datos. ¿Quiénes serán los propietarios de estos datos?\n",
    "4. **Infraestructura Autónoma:** Proporciona una visión general de cómo cada dominio manejará su infraestructura para la ingestión, almacenamiento, procesamiento y exposición de datos.\n",
    "5. **Interoperabilidad:** Define cómo los distintos dominios se comunicarán y compartirán datos entre sí. ¿Qué estándares y protocolos se utilizarán?\n",
    "6. **Gobernanza:** Establece principios de gobernanza de datos que aseguren la calidad, seguridad y cumplimiento de los datos en todos los dominios.\n",
    "7. **Tecnologías Utilizadas:** Identifica las tecnologías y herramientas que emplearás para implementar tu diseño de arquitectura de datos (por ejemplo, Apache Spark, Kafka, Delta Lake, etc.).\n",
    "8. **Evolución y Escalabilidad:** Proporciona una estrategia para evolucionar y escalar la arquitectura de datos conforme crezca la organización y la cantidad de datos.\n",
    "\n",
    "**Entregable:**\n",
    "Un documento en formato PDF que incluya un diagrama arquitectónico (usa la aplicación que consideres, por ejemplo, https://excalidraw.com/ ) y la descripción detallada de los puntos mencionados anteriormente. Además, justifica por qué elegiste este diseño y cómo crees que cumplirá con los requisitos del sistema de inspecciones de sanidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "408b72e8-634e-456f-8719-7b8ba2656a24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1554611121301019,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Proyecto Apache Spark - Jorge de Dios",
   "widgets": {}
  },
  "colab": {
   "collapsed_sections": [
    "Jq9d0x1OTh2N",
    "8Z0h3dF9Vg4X"
   ],
   "name": "Evaluacion_Apache_Spark_Solutions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
