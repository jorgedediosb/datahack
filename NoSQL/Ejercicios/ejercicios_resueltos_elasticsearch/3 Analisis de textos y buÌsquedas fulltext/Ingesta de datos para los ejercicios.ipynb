{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesta de datos para los ejercicios\n",
    "\n",
    "Vamos a crear el íncide que utilizaremos en los ejercicios e ingestar los datos necesarios para poder ejecutar las consultas.\n",
    "\n",
    "Vamos a almacenar recetas de cocina. Lo primero que vamos a ver es el formato de los datos que vamos a insertar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"../../data/elasticsearch/recipes/recipes_copia.json\", lines = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1: Crear el índice\n",
    "\n",
    "Para crear el índice ejecuta la siguiente sentencia usando la herramienta dev tools de Kibana.\n",
    "\n",
    "```\n",
    "PUT recipes\n",
    "{\n",
    "    \"aliases\": {},\n",
    "    \"settings\" : { \n",
    "        \"index\" : {\n",
    "            \"number_of_shards\" : 4,\n",
    "            \"number_of_replicas\" : 1\n",
    "        },\n",
    "        \"analysis\": {\n",
    "          \"analyzer\": {\n",
    "            \"autocomplete\": {\n",
    "              \"tokenizer\": \"autocomplete\",\n",
    "              \"filter\": [\n",
    "                \"lowercase\"\n",
    "              ]\n",
    "            },\n",
    "            \"autocomplete_search\": {\n",
    "              \"tokenizer\": \"lowercase\"\n",
    "            }\n",
    "          },\n",
    "          \"tokenizer\": {\n",
    "            \"autocomplete\": {\n",
    "              \"type\": \"ngram\",\n",
    "              \"min_gram\": 1,\n",
    "              \"max_gram\": 2,\n",
    "              \"token_chars\": [\n",
    "                \"letter\"\n",
    "              ]\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "      \"properties\": {\n",
    "        \"author\": {\n",
    "          \"type\": \"keyword\",\n",
    "          \"eager_global_ordinals\": true,\n",
    "          \"fields\": {\n",
    "            \"text\": {\n",
    "              \"type\": \"text\"\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"date\": {\n",
    "          \"type\": \"date\",\n",
    "          \"format\": \"[MMMM yyyy]\",\n",
    "          \"fields\": {\n",
    "            \"keyword\": {\n",
    "              \"type\": \"keyword\"\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"description\": {\n",
    "          \"type\": \"text\",\n",
    "          \"analyzer\": \"english\"\n",
    "        },\n",
    "        \"ingredients\": {\n",
    "          \"type\": \"text\",\n",
    "          \"analyzer\": \"english\"\n",
    "        },\n",
    "        \"instructions\": {\n",
    "          \"type\": \"text\",\n",
    "          \"analyzer\": \"english\"\n",
    "        },\n",
    "        \"picture_link\": {\n",
    "          \"type\": \"keyword\"\n",
    "        },\n",
    "        \"rating\": {\n",
    "          \"properties\": {\n",
    "            \"bestRating\": {\n",
    "              \"type\": \"float\"\n",
    "            },\n",
    "            \"prepareAgainPct\": {\n",
    "              \"type\": \"float\"\n",
    "            },\n",
    "            \"ratingValue\": {\n",
    "              \"type\": \"float\"\n",
    "            },\n",
    "            \"worstRating\": {\n",
    "              \"type\": \"float\"\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"recipe_id\": {\n",
    "          \"type\": \"text\",\n",
    "          \"fields\": {\n",
    "            \"keyword\": {\n",
    "              \"type\": \"keyword\",\n",
    "              \"ignore_above\": 256\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"summary\": {\n",
    "          \"properties\": {\n",
    "            \"active-time\": {\n",
    "              \"type\": \"keyword\"\n",
    "            },\n",
    "            \"total-time\": {\n",
    "              \"type\": \"keyword\"\n",
    "            },\n",
    "            \"yield\": {\n",
    "              \"type\": \"keyword\"\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"title\": {\n",
    "          \"type\": \"text\",\n",
    "          \"analyzer\": \"english\",\n",
    "          \"fields\": {\n",
    "            \"keyword\": {\n",
    "              \"type\": \"keyword\"\n",
    "            },\n",
    "            \"suggestion\": {\n",
    "              \"type\": \"completion\",\n",
    "              \"analyzer\": \"english\",\n",
    "              \"preserve_separators\": false,\n",
    "              \"preserve_position_increments\": false,\n",
    "              \"max_input_length\": 50\n",
    "            },\n",
    "            \"ngram\": {\n",
    "            \t\"type\": \"text\",\n",
    "            \t\"analyzer\": \"autocomplete\",\n",
    "        \t\t\"search_analyzer\": \"autocomplete_search\"\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"url\": {\n",
    "          \"type\": \"text\",\n",
    "          \"fields\": {\n",
    "            \"keyword\": {\n",
    "              \"type\": \"keyword\",\n",
    "              \"ignore_above\": 256\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2: Levantar Logstash\n",
    "\n",
    "Vamos a ingestar los datos usando Logsetash usando una imagen de docker para ejecutarlo:\n",
    "\n",
    "* Para que pueda encontrar el servicio de Elasticsearch vamos a añadir el conteneror a la red de nuestro laboratorio, `--network=datahack-elk-v1_defaul`. Puedes consultar la red creada por tu compose usando el comando `docker networks ls`\n",
    "* Montamos el volumen donde se encuentra nuestro fichero con el pipeline y los referenciamos a la carpeta del contenedor donde Logstash espera encontrar esa configuración, `-v /Users/rgarrote/desarrollo/datahack-elk-v1/work/data/elasticsearch/web_logs/pipeline/:/usr/share/logstash/pipeline/`.\n",
    "* Montamos el volumen donde dejaremos los ficheros de log a parsear. `-v /Users/rgarrote/desarrollo/datahack-elk-v1/work/data/elasticsearch/web_logs/data/:/tmp/data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run --rm -it --network=datahack-elk-v1_defaul \\\n",
    "    -v /Users/rgarrote/desarrollo/datahack-elk-v1/work/data/elasticsearch/recipies/pipeline/:/usr/share/logstash/pipeline/ \\\n",
    "    -v /Users/rgarrote/desarrollo/datahack-elk-v1/work/data/elasticsearch/recipies/data/:/tmp/data/ \\\n",
    "docker.elastic.co/logstash/logstash:8.3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 3: Ingestar los datos.\n",
    "\n",
    "Una vez que Logstash haya levantado y esté listo para procesar ficheros, copia el fichero que encontrarás en la ruta `work/data/elasticsearch/recipes/recipes.json` en la carpeta `workdata/elasticsearch/recipies/data`.\n",
    "\n",
    "Por cada documento ingestado se mostrará un punto en la pantalla.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 4: Comprobar que el proceso se está realizando correctamente\n",
    "\n",
    "1. Comprueba en Kibana que se ha ceado el íncide recipes.\n",
    "2. Desde la sección de Index Management averigua cuantas recetas se han insertado en el índice recipes de Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
